{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import hstack\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import rand\n",
    "from numpy.random import randn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from keras.initializers.initializers_v1 import RandomNormal\n",
    "from keras.backend import clear_session \n",
    "import numpy as np\n",
    "from keras import Model\n",
    "from random import shuffle\n",
    "from keras.layers import Dropout,Dense,Flatten,BatchNormalization,LeakyReLU,Embedding,multiply\n",
    "from keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cGAN():\n",
    "    \n",
    "    \"\"\"\n",
    "    Class containing 3 methods (and __init__): generator, discriminator and train.\n",
    "    Generator is trained using random noise and label as inputs. Discriminator is trained\n",
    "    using real/fake samples and labels as inputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,latent_dim=32, out_shape=14):\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.out_shape = out_shape \n",
    "        self.num_classes = 2\n",
    "        # using Adam as our optimizer\n",
    "        #optimizer = Adam(0.0002, 0.5)\n",
    "        \n",
    "        # building the discriminator\n",
    "        self.discriminator = self.discriminator()\n",
    "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
    "                                   optimizer=\"Adam\",\n",
    "                                   metrics=['accuracy'])\n",
    "\n",
    "        # building the generator\n",
    "        self.generator = self.generator()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,))\n",
    "        gen_samples = self.generator([noise, label])\n",
    "        \n",
    "        # we don't train discriminator when training generator\n",
    "        self.discriminator.trainable = False\n",
    "        valid = self.discriminator([gen_samples, label])\n",
    "\n",
    "        # combining both models\n",
    "        self.combined = Model([noise, label], valid)\n",
    "        self.combined.compile(loss=['binary_crossentropy'],\n",
    "                              optimizer=\"Adam\",\n",
    "                             metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    def generator(self):\n",
    "        init = RandomNormal(mean=0.0, stddev=0.02)\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128, input_dim=self.latent_dim))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "        model.add(Dense(256))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "        model.add(Dense(512))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "        model.add(Dense(self.out_shape, activation='tanh'))\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
    "        \n",
    "        model_input = multiply([noise, label_embedding])\n",
    "        gen_sample = model(model_input)\n",
    "\n",
    "        return Model([noise, label], gen_sample, name=\"Generator\")\n",
    "\n",
    "    \n",
    "    def discriminator(self):\n",
    "        init = RandomNormal(mean=0.0, stddev=0.02)\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(512, input_dim=self.out_shape, kernel_initializer=init))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(Dense(256, kernel_initializer=init))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        \n",
    "        model.add(Dense(128, kernel_initializer=init))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        \n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        gen_sample = Input(shape=(self.out_shape,))\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        label_embedding = Flatten()(Embedding(self.num_classes, self.out_shape)(label))\n",
    "\n",
    "        model_input = multiply([gen_sample, label_embedding])\n",
    "        validity = model(model_input)\n",
    "\n",
    "        return Model(inputs=[gen_sample, label], outputs=validity, name=\"Discriminator\")\n",
    "\n",
    "\n",
    "    def train(self, X_train, y_train, pos_index, neg_index, epochs, sampling=False, batch_size=32, sample_interval=100, plot=True): \n",
    "        \n",
    "        # though not recommended, defining losses as global helps as in analysing our cgan out of the class\n",
    "        global G_losses\n",
    "        global D_losses\n",
    "        \n",
    "        G_losses = []\n",
    "        D_losses = []\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # if sampling==True --> train discriminator with 8 sample from postivite class and rest with negative class\n",
    "            if sampling:\n",
    "                idx1 = np.random.choice(pos_index, 8)\n",
    "                idx0 = np.random.choice(neg_index, batch_size-8)\n",
    "                idx = np.concatenate((idx1, idx0))\n",
    "            # if sampling!=True --> train discriminator using random instances in batches of 32\n",
    "            else:\n",
    "                idx = np.random.choice(len(y_train), batch_size)\n",
    "            samples, labels = X_train[idx], y_train[idx]\n",
    "            samples, labels = shuffle(samples, labels)\n",
    "            \n",
    "            # Sample noise as generator input\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            gen_samples = self.generator.predict([noise, labels])\n",
    "\n",
    "            # label smoothing\n",
    "            if epoch < epochs//1.5:\n",
    "                valid_smooth = (valid+0.1)-(np.random.random(valid.shape)*0.1)\n",
    "                fake_smooth = (fake-0.1)+(np.random.random(fake.shape)*0.1)\n",
    "            else:\n",
    "                valid_smooth = valid \n",
    "                fake_smooth = fake\n",
    "                \n",
    "            # Train the discriminator\n",
    "            self.discriminator.trainable = True\n",
    "            d_loss_real = self.discriminator.train_on_batch([samples, labels], valid_smooth)\n",
    "            d_loss_fake = self.discriminator.train_on_batch([gen_samples, labels], fake_smooth)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # Train Generator\n",
    "            self.discriminator.trainable = False\n",
    "            sampled_labels = np.random.randint(0, 2, batch_size).reshape(-1, 1)\n",
    "            # Train the generator\n",
    "            g_loss = self.combined.train_on_batch([noise, sampled_labels], valid)\n",
    "\n",
    "            if (epoch+1)%sample_interval==0:\n",
    "                print('[%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f'\n",
    "                  % (epoch, epochs, d_loss[0], g_loss[0]))\n",
    "            G_losses.append(g_loss[0])\n",
    "            D_losses.append(d_loss[0])\n",
    "            if plot:\n",
    "                if epoch+1==epochs:\n",
    "                    plt.figure(figsize=(10,5))\n",
    "                    plt.title(\"Generator and Discriminator Loss\")\n",
    "                    plt.plot(G_losses,label=\"G\")\n",
    "                    plt.plot(D_losses,label=\"D\")\n",
    "                    plt.xlabel(\"iterations\")\n",
    "                    plt.ylabel(\"Loss\")\n",
    "                    plt.legend()\n",
    "                    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan=cGAN()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
